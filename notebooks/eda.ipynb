{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62764667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5accf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.utils.data_utils import prepare_dataset, extract_training_split_from_filename, extract_anomaly_range_from_filename, create_file_mapping\n",
    "\n",
    "def visualize_dataset_eda(file_path):\n",
    "    \"\"\"\n",
    "    Visualizes the time series dataset with highlighted training and anomaly regions for EDA.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the dataset file.\n",
    "    \"\"\"\n",
    "    data = prepare_dataset(file_path)\n",
    "    training_split = extract_training_split_from_filename(os.path.basename(file_path))\n",
    "    anomaly_range = extract_anomaly_range_from_filename(os.path.basename(file_path))\n",
    "    anomaly_start, anomaly_end = anomaly_range\n",
    "\n",
    "    plt.figure(figsize=(18, 7))\n",
    "    plt.plot(data[\"Value\"], label=\"Time Series Data\", color=\"navy\")\n",
    "\n",
    "    plt.axvspan(0, training_split, color=\"lightgreen\", alpha=0.4, label=f\"Training Data (0 to {training_split})\")\n",
    "    plt.axvspan(anomaly_start, anomaly_end, color=\"salmon\", alpha=0.6, label=f\"Anomaly ({anomaly_start} to {anomaly_end})\")\n",
    "\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(f\"Time Series Visualization: {os.path.basename(file_path)}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    zoom_padding = (anomaly_end - anomaly_start) * 2\n",
    "    zoom_start_padded = max(0, int(anomaly_start - zoom_padding))\n",
    "    zoom_end_padded = min(len(data), int(anomaly_end + zoom_padding))\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(range(zoom_start_padded, zoom_end_padded), data[\"Value\"].iloc[zoom_start_padded:zoom_end_padded], label=\"Time Series Data\", color=\"navy\", marker='o', markersize=3, linestyle='-')\n",
    "    plt.axvspan(anomaly_start, anomaly_end, color=\"salmon\", alpha=0.9, label=\"Ground Truth Anomaly\")\n",
    "    \n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(f\"Zoomed-in Anomaly Region: {os.path.basename(file_path)}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data_folder = 'data'\n",
    "file_mapping = create_file_mapping(data_folder)\n",
    "\n",
    "if file_mapping:\n",
    "    first_dataset_key = sorted(file_mapping.keys())[0]\n",
    "    print(f\"Visualizing dataset for key: {first_dataset_key} - {file_mapping[first_dataset_key]}\")\n",
    "    visualize_dataset_eda(file_mapping[first_dataset_key])\n",
    "else:\n",
    "    print(f\"No .txt files found in {data_folder}. Please check the path and ensure data files are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(file_mapping.keys())[:10]:\n",
    "    print(f\"Visualizing dataset for key: {key}\")\n",
    "    visualize_dataset_eda(file_mapping[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae810c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detector_output(data, anomaly_scores, detected_anomaly_indices, anomaly_range, detector_name=\"Detector\"):\n",
    "    \"\"\"\n",
    "    Visualizes the time series with anomaly scores and detected anomaly points.\n",
    "    \"\"\"\n",
    "    anomaly_start, anomaly_end = anomaly_range\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(18, 7))\n",
    "\n",
    "    color = 'navy'\n",
    "    ax1.set_xlabel('Time Index')\n",
    "    ax1.set_ylabel('Value', color=color)\n",
    "    ax1.plot(data.index, data[\"Value\"], color=color, label='Time Series Data')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.axvspan(anomaly_start, anomaly_end, color=\"salmon\", alpha=0.5, label=\"Ground Truth Anomaly\")\n",
    "    \n",
    "    if detected_anomaly_indices is not None and len(detected_anomaly_indices) > 0:\n",
    "        valid_indices = [idx for idx in detected_anomaly_indices if idx in data.index]\n",
    "        if valid_indices:\n",
    "            ax1.plot(data.loc[valid_indices].index, data[\"Value\"].loc[valid_indices], 'ro', markersize=6, label=f'{detector_name} Detected Anomalies')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'darkorange'\n",
    "    ax2.set_ylabel('Anomaly Score', color=color)\n",
    "\n",
    "    ax2.plot(data.index, anomaly_scores, color=color, alpha=0.7, label=f'{detector_name} Anomaly Scores')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(f'{detector_name} Output Visualization')\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.models.traditional.local_outlier_factor import LOFDetector\n",
    "from src.utils.data_utils import prepare_dataset, extract_training_split_from_filename, extract_anomaly_range_from_filename\n",
    "\n",
    "print(\"Visualizing LOF Detector Output for the first 5 datasets...\")\n",
    "\n",
    "if 'file_mapping' not in globals() or not file_mapping:\n",
    "    print(\"Error: file_mapping not found or is empty. Please run the cell that creates file_mapping first.\")\n",
    "elif 'visualize_detector_output' not in globals():\n",
    "    print(\"Error: visualize_detector_output function not defined. Please run the cell where it's defined.\")\n",
    "else:\n",
    "    lof_detector = LOFDetector() \n",
    "    threshold_value = 4\n",
    "\n",
    "    for i, key in enumerate(sorted(file_mapping.keys())[:5]):\n",
    "        file_path = file_mapping[key]\n",
    "        dataset_basename = os.path.basename(file_path)\n",
    "        print(f\"\\nProcessing dataset {i+1}/5: {dataset_basename} (key: {key})\")\n",
    "\n",
    "        try:\n",
    "            data_df = prepare_dataset(file_path)\n",
    "            if data_df.empty:\n",
    "                print(f\"  Skipping {dataset_basename}, data is empty.\")\n",
    "                continue\n",
    "\n",
    "            training_split_idx = extract_training_split_from_filename(dataset_basename)\n",
    "            anomaly_range = extract_anomaly_range_from_filename(dataset_basename)\n",
    "\n",
    "            train_values = data_df['Value'].iloc[:training_split_idx].values.reshape(-1, 1)\n",
    "            test_values = data_df['Value'].iloc[training_split_idx:].values.reshape(-1, 1)\n",
    "\n",
    "            padded_scores = np.full(len(data_df), np.nan)\n",
    "            all_detected_anomaly_abs_indices = []\n",
    "\n",
    "            if test_values.shape[0] == 0:\n",
    "                print(f\"  Skipping {dataset_basename}, no test data after split.\")\n",
    "            else:\n",
    "                min_train_samples = getattr(lof_detector, 'n_neighbors', 20) \n",
    "                if train_values.shape[0] < min_train_samples:\n",
    "                    print(f\"  Warning: Not enough training samples ({train_values.shape[0]}) for LOF (needs at least {min_train_samples}). Scores will not be generated.\")\n",
    "                else:\n",
    "                    print(f\"  Fitting LOF on {train_values.shape[0]} training samples...\")\n",
    "                    lof_detector.fit(train_values)\n",
    "\n",
    "                    print(f\"  Scoring with LOF on {test_values.shape[0]} test samples...\")\n",
    "                    anomaly_scores_test = lof_detector.score(test_values)\n",
    "                    \n",
    "                    if not isinstance(anomaly_scores_test, np.ndarray) or anomaly_scores_test.shape[0] != test_values.shape[0]:\n",
    "                        print(f\"  Warning: Anomaly scores shape mismatch for {dataset_basename}. Expected {test_values.shape[0]}, got {anomaly_scores_test.shape if isinstance(anomaly_scores_test, np.ndarray) else 'N/A'}.\")\n",
    "                    elif anomaly_scores_test.size == 0:\n",
    "                         print(f\"  Warning: LOF returned empty scores for {dataset_basename}.\")\n",
    "                    else:\n",
    "                        anomaly_scores_test_flat = anomaly_scores_test.flatten()\n",
    "                        padded_scores[training_split_idx : training_split_idx + len(anomaly_scores_test_flat)] = anomaly_scores_test_flat\n",
    "\n",
    "                        mean_score = np.mean(anomaly_scores_test_flat)\n",
    "                        std_score = np.std(anomaly_scores_test_flat)\n",
    "                        threshold = mean_score + threshold_value * std_score\n",
    "                        binary_predictions_test = (anomaly_scores_test_flat > threshold).astype(int)\n",
    "                        \n",
    "                        if binary_predictions_test.size > 0:\n",
    "                            detected_indices_in_test = np.where(binary_predictions_test == 1)[0]\n",
    "                            if detected_indices_in_test.size > 0:\n",
    "                                all_detected_anomaly_abs_indices = [training_split_idx + idx for idx in detected_indices_in_test]\n",
    "                                print(f\"  {len(all_detected_anomaly_abs_indices)} anomalies detected by LOF at indices (relative to full data): {all_detected_anomaly_abs_indices}\")\n",
    "                            else:\n",
    "                                print(f\"  No anomaly detected by LOF with current threshold.\")\n",
    "                        else:\n",
    "                            print(f\"  No binary predictions generated from scores.\")\n",
    "            \n",
    "            visualize_detector_output(\n",
    "                data=data_df, \n",
    "                anomaly_scores=padded_scores, \n",
    "                detected_anomaly_indices=all_detected_anomaly_abs_indices,\n",
    "                anomaly_range=anomaly_range, \n",
    "                detector_name=\"LOF\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing dataset {dataset_basename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
